# üèóÔ∏è Production Architecture for 512MB RAM Constraint

## Problem
Render free tier = 512MB RAM, but ML models need 2-8GB RAM.

## Solution
Separate lightweight API gateway from heavy ML inference.

---

## üéØ Recommended Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FRONTEND LAYER                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   React + Vite (Vercel/Netlify FREE)                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - User interface                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - File upload                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - Results display                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚îÇ HTTPS/REST
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    API GATEWAY LAYER                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Lightweight Gateway (Render Free - 512MB)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   FastAPI or Express.js                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - Authentication/Authorization                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - Request routing                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - Rate limiting                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - Response caching (Redis/Upstash)                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - NO ML MODELS HERE                               ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚îÇ Routes to appropriate service
               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ML INFERENCE LAYER                        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ  Hugging Face      ‚îÇ  ‚îÇ  Modal.com         ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  Inference API     ‚îÇ  ‚îÇ  Serverless GPU    ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ                    ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Pre-trained     ‚îÇ  ‚îÇ  ‚Ä¢ Custom models   ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ    models          ‚îÇ  ‚îÇ  ‚Ä¢ Your detectors  ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ FREE tier       ‚îÇ  ‚îÇ  ‚Ä¢ Auto-scaling    ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ 1k req/day      ‚îÇ  ‚îÇ  ‚Ä¢ Pay per use     ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Alternative: Replicate, RunPod, AWS Lambda + EFS           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚îÇ Store/retrieve data
               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATA LAYER                                ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  PostgreSQL     ‚îÇ  ‚îÇ  S3/R2/Backblaze‚îÇ  ‚îÇ  Upstash   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (Supabase)     ‚îÇ  ‚îÇ  (File Storage) ‚îÇ  ‚îÇ  (Redis)   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ User data    ‚îÇ  ‚îÇ  ‚Ä¢ Trained      ‚îÇ  ‚îÇ  ‚Ä¢ Cache   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Predictions  ‚îÇ  ‚îÇ    models       ‚îÇ  ‚îÇ  ‚Ä¢ Session ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Logs         ‚îÇ  ‚îÇ  ‚Ä¢ Datasets     ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚îÇ Scheduled tasks
               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AUTOMATION LAYER                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  GitHub Actions (FREE)                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Model training jobs (monthly)                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Data pipeline                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Backup & cleanup                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Implementation Options

### **Option 1: Hugging Face + Render Gateway (Simplest)**

**Pros:**
- ‚úÖ Minimal code changes
- ‚úÖ FREE tier: 1,000 requests/day
- ‚úÖ No infrastructure management
- ‚úÖ Pre-trained models work out-of-box

**Cons:**
- ‚ùå Can't use your custom trained detectors easily
- ‚ùå Limited model selection
- ‚ùå Slower for custom embeddings

**Best for:** MVP, testing, demos

---

### **Option 2: Modal.com + Render Gateway (Recommended)**

**Pros:**
- ‚úÖ Use your exact training code
- ‚úÖ Load custom trained models
- ‚úÖ Auto-scaling (0 to infinity)
- ‚úÖ Only pay when running (~$0.10/1000 requests)
- ‚úÖ Keep your current model architecture

**Cons:**
- ‚ùå Small learning curve
- ‚ùå Cold starts (5-10s first request, then fast)

**Best for:** Production with custom models

**Cost estimate:**
- 10,000 requests/month ‚âà **$1-2/month**
- 100,000 requests/month ‚âà **$10-20/month**

---

### **Option 3: AWS Lambda + EFS (Advanced)**

**Pros:**
- ‚úÖ Generous free tier (1M requests/month)
- ‚úÖ Load models from EFS
- ‚úÖ Established platform

**Cons:**
- ‚ùå Complex setup
- ‚ùå 10GB Lambda limit
- ‚ùå Cold starts

**Best for:** High-scale production (>1M requests/month)

---

## üìù Migration Path for Your Current Code

### Step 1: Keep Gateway Lightweight (Render Free Tier)

**File: `services/gateway_lite/main.py`**
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx
import os

app = FastAPI(title="Deepfake Gateway")

MODAL_WEBHOOK_URL = os.getenv("MODAL_WEBHOOK_URL")

class AnalyzeRequest(BaseModel):
    text: str
    model_name: str = "Qwen/Qwen2.5-0.5B"
    layer: int = 22
    classifier_type: str = "svm"

@app.post("/analyze")
async def analyze(req: AnalyzeRequest):
    """Route to ML service (Modal/HF)"""
    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.post(
            MODAL_WEBHOOK_URL,
            json=req.dict()
        )
        return response.json()

@app.get("/health")
def health():
    return {"status": "ok", "ram_usage": "~50MB"}
```

**Memory usage: ~50-100MB** ‚úÖ

---

### Step 2: Deploy ML Models to Modal

**File: `modal_deployment/detector.py`**
```python
import modal
from pathlib import Path

# Create Modal app
stub = modal.Stub("deepfake-text-detector")

# Define the image with your dependencies
image = (
    modal.Image.debian_slim()
    .pip_install(
        "torch",
        "transformers",
        "scikit-learn",
        "sentence-transformers",
        "numpy",
        "pandas"
    )
)

# Mount your trained models
models_volume = modal.NetworkFileSystem.from_name(
    "deepfake-models", 
    create_if_missing=True
)

@stub.function(
    image=image,
    gpu="T4",  # Optional: remove for CPU-only
    memory=4096,  # 4GB RAM
    timeout=300,
    network_file_systems={"/models": models_volume}
)
def detect_deepfake(
    text: str,
    model_name: str,
    layer: int,
    classifier_type: str
):
    """
    Your existing detection logic - no changes needed!
    """
    import sys
    sys.path.append("/models")
    
    from extractors import EmbeddingExtractor
    from classifiers import BinaryDetector
    import pickle
    
    # Load your trained detector
    detector_path = f"/models/detector_{model_name.replace('/', '_')}_layer{layer}_{classifier_type}.pkl"
    with open(detector_path, "rb") as f:
        detector = pickle.load(f)
    
    # Extract features
    extractor = EmbeddingExtractor(model_name, device="cuda")
    features = extractor.get_pooled_layer_embeddings(
        [text],
        layer_idx=layer,
        pooling="mean"
    )
    
    # Predict
    pred, prob = detector.predict(features, return_probabilities=True)
    
    return {
        "prediction": int(pred[0]),
        "probability": float(prob[0]),
        "is_fake": bool(pred[0] == 1)
    }

# Web endpoint
@stub.webhook(method="POST")
def webhook(data: dict):
    result = detect_deepfake.remote(
        text=data["text"],
        model_name=data["model_name"],
        layer=data["layer"],
        classifier_type=data.get("classifier_type", "svm")
    )
    return result
```

**Deploy:**
```bash
# Install Modal CLI
pip install modal

# Authenticate
modal token new

# Deploy
modal deploy modal_deployment/detector.py

# Get your webhook URL (copy this to gateway env vars)
# https://yourusername--deepfake-text-detector-webhook.modal.run
```

---

### Step 3: Upload Your Trained Models to Modal

```bash
# One-time setup: upload your models
modal volume put deepfake-models saved_models/
```

Or via Python:
```python
import modal

# Get the volume
volume = modal.NetworkFileSystem.lookup("deepfake-models")

# Upload trained models
with volume.batch_upload() as upload:
    upload.put_directory("saved_models/", "/")
```

---

## üí∞ Cost Comparison

| Solution | Free Tier | Cost (10k req/mo) | Cost (100k req/mo) |
|----------|-----------|-------------------|-------------------|
| **HF Inference** | 1k/day | FREE | $50-100 |
| **Modal.com** | - | $1-2 | $10-20 |
| **Replicate** | - | $5 | $50 |
| **RunPod** | - | $4 | $40 |
| **AWS Lambda** | 1M free | FREE | FREE-$10 |
| **Render 2GB** | - | $7/mo | $7/mo (but limited) |

---

## üöÄ Recommended Setup for You

### Phase 1: MVP (Start Here)
```
Frontend (Vercel) ‚Üí Gateway (Render Free) ‚Üí HF Inference API
                          ‚Üì
                   PostgreSQL (Supabase)
```
**Cost:** FREE for <1k requests/day

### Phase 2: Production (Custom Models)
```
Frontend (Vercel) ‚Üí Gateway (Render Free) ‚Üí Modal.com
                          ‚Üì
                   PostgreSQL + S3
```
**Cost:** ~$1-5/month for 10k-50k requests

### Phase 3: Scale (>100k requests/month)
```
Frontend (Vercel) ‚Üí Gateway (Render Starter) ‚Üí Modal.com + Cache
                          ‚Üì
                   PostgreSQL + Redis + S3
```
**Cost:** ~$20-30/month

---

## üì¶ What to Store Where

### Render Gateway (512MB)
- ‚úÖ API routing logic
- ‚úÖ Authentication
- ‚úÖ Input validation
- ‚úÖ Response caching (in-memory dict for last 100 results)
- ‚ùå NO ML models
- ‚ùå NO large datasets

### Modal.com / HF
- ‚úÖ All ML models
- ‚úÖ Embedding extractors
- ‚úÖ Trained classifiers
- ‚úÖ Feature computation

### S3 / Cloudflare R2
- ‚úÖ Trained model files (.pkl, .pt)
- ‚úÖ Training datasets
- ‚úÖ User uploads (if needed)

### PostgreSQL (Supabase)
- ‚úÖ User accounts
- ‚úÖ Prediction history
- ‚úÖ API usage logs
- ‚úÖ Model metadata

---

## üîÑ Migration Checklist

- [ ] Create Modal account (free to start)
- [ ] Deploy detector to Modal using provided code
- [ ] Upload trained models to Modal volume
- [ ] Update gateway to call Modal webhook
- [ ] Add Modal webhook URL to Render env vars
- [ ] Test end-to-end flow
- [ ] Remove ML models from Render service
- [ ] Deploy lightweight gateway
- [ ] Monitor costs and performance

---

## üÜò Need Help?

I can help you:
1. Set up Modal deployment with your existing code
2. Migrate specific detectors
3. Optimize for cost/performance
4. Set up caching to reduce API calls

Just let me know which option you want to pursue!
